

import torch
import torchvision
import torch.nn as nn
from torch.utils.data import Dataset
import torch.nn.functional as F


norm = torchvision.transforms.Normalize(mean=[0.5187, 0.5181, 0.5186, 0.5198, 0.5175, 0.5171, 0.5130, 0.5139, 0.5155,
                                                0.5211, 0.5202, 0.5232, 0.5243, 0.5234, 0.5214, 0.5225, 0.5210, 0.5246,
                                                0.5237, 0.5278, 0.5272, 0.5299, 0.5290, 0.5288, 0.5319, 0.5443, 0.5545,
                                                0.5548, 0.5563, 0.5499, 0.5597, 0.5577, 0.5574, 0.5546, 0.5508, 0.5550,
                                                0.5639, 0.5638, 0.5638, 0.5637, 0.5659, 0.5652, 0.5726, 0.5669, 0.5662,
                                                0.5622, 0.5602, 0.5632, 0.5642, 0.5600, 0.5668, 0.5741, 0.5763, 0.5744,
                                                0.5741, 0.5849, 0.5850, 0.5837, 0.5840, 0.5838, 0.5725, 0.5719, 0.5788,
                                                0.5875, 0.5913, 0.6010, 0.6037, 0.6027, 0.5975, 0.6078, 0.6078, 0.6070,
                                                0.6180, 0.6081, 0.5998, 0.6032, 0.6043, 0.6037, 0.6160, 0.6178, 0.6220,
                                                0.6117, 0.6102, 0.6055, 0.6058, 0.5967, 0.5964, 0.5966, 0.5982, 0.5971,
                                                0.5968, 0.5992, 0.5932, 0.5926, 0.5923, 0.5895, 0.5895, 0.6034, 0.5728,
                                                0.5720, 0.1764, 0.1767, 0.1679, 0.1731, 0.1690, 0.1775, 0.1790, 0.1747,
                                                0.1743, 0.1713, 0.1601, 0.1768, 0.1806, 0.1936, 0.1896, 0.1897, 0.1780,
                                                0.1866, 0.1845, 0.1915, 0.1893, 0.2102, 0.1986, 0.1941, 0.2017, 0.2244,
                                                0.2435, 0.2426, 0.2498, 0.2481, 0.2530, 0.2583, 0.2554, 0.2463, 0.2382,
                                                0.2419, 0.2577, 0.2531, 0.2521, 0.2546, 0.2550, 0.2467, 0.2621, 0.2569,
                                                0.2546, 0.2513, 0.2381, 0.2379, 0.2449, 0.2371, 0.2584, 0.2653, 0.2718,
                                                0.2561, 0.2514, 0.2509, 0.2508, 0.2431, 0.2435, 0.2373, 0.2267, 0.2270,
                                                0.2362, 0.2586, 0.2767, 0.2884, 0.2832, 0.2768, 0.2613, 0.2846, 0.2801,
                                                0.2683, 0.2726, 0.2713, 0.2525, 0.2732, 0.2836, 0.2736, 0.3007, 0.2967,
                                                0.3162, 0.3074, 0.3080, 0.2919, 0.2870, 0.2564, 0.2522, 0.2513, 0.2530,
                                                0.2493, 0.2438, 0.2523, 0.2251, 0.2118, 0.2062, 0.2006, 0.2001, 0.2410,
                                                0.1797, 0.1750, 0.0926, 0.0950, 0.0927, 0.0970, 0.0951, 0.0994, 0.0956,
                                                0.0934, 0.0909, 0.0832, 0.0747, 0.0810, 0.0820, 0.0897, 0.0905, 0.0925,
                                                0.0860, 0.0917, 0.0934, 0.0908, 0.0905, 0.0964, 0.0922, 0.0890, 0.0935,
                                                0.0902, 0.0931, 0.0918, 0.0950, 0.0974, 0.1001, 0.0960, 0.0905, 0.0826,
                                                0.0807, 0.0816, 0.0867, 0.0843, 0.0842, 0.0868, 0.0821, 0.0757, 0.0718,
                                                0.0730, 0.0719, 0.0730, 0.0663, 0.0656, 0.0745, 0.0748, 0.0837, 0.0820,
                                                0.0709, 0.0665, 0.0583, 0.0517, 0.0516, 0.0516, 0.0511, 0.0488, 0.0504,
                                                0.0504, 0.0479, 0.0527, 0.0560, 0.0552, 0.0419, 0.0392, 0.0342, 0.0384,
                                                0.0372, 0.0346, 0.0329, 0.0360, 0.0344, 0.0423, 0.0470, 0.0417, 0.0438,
                                                0.0422, 0.0418, 0.0423, 0.0442, 0.0419, 0.0400, 0.0361, 0.0350, 0.0348,
                                                0.0347, 0.0343, 0.0329, 0.0328, 0.0286, 0.0166, 0.0142, 0.0146, 0.0156,
                                                0.0181, 0.0122, 0.0107, 0.5000],
                                        std=[0.4997, 0.4997, 0.4997, 0.4996, 0.4997, 0.4997, 0.4998, 0.4998, 0.4998,
                                                0.4996, 0.4996, 0.4995, 0.4994, 0.4995, 0.4995, 0.4995, 0.4996, 0.4994,
                                                0.4994, 0.4992, 0.4993, 0.4991, 0.4992, 0.4992, 0.4990, 0.4980, 0.4970,
                                                0.4970, 0.4968, 0.4975, 0.4964, 0.4967, 0.4967, 0.4970, 0.4974, 0.4970,
                                                0.4959, 0.4959, 0.4959, 0.4959, 0.4956, 0.4957, 0.4947, 0.4955, 0.4956,
                                                0.4961, 0.4964, 0.4960, 0.4959, 0.4964, 0.4955, 0.4945, 0.4941, 0.4944,
                                                0.4945, 0.4927, 0.4927, 0.4929, 0.4929, 0.4929, 0.4947, 0.4948, 0.4938,
                                                0.4923, 0.4916, 0.4897, 0.4891, 0.4893, 0.4904, 0.4882, 0.4883, 0.4884,
                                                0.4859, 0.4882, 0.4899, 0.4892, 0.4890, 0.4891, 0.4864, 0.4859, 0.4849,
                                                0.4874, 0.4877, 0.4887, 0.4887, 0.4906, 0.4906, 0.4906, 0.4903, 0.4905,
                                                0.4905, 0.4901, 0.4912, 0.4914, 0.4914, 0.4919, 0.4919, 0.4892, 0.4947,
                                                0.4948, 0.9776, 0.9715, 0.9727, 0.9692, 0.9636, 0.9577, 0.9621, 0.9558,
                                                0.9514, 0.9569, 0.9544, 0.9578, 0.9524, 0.9475, 0.9400, 0.9363, 0.9328,
                                                0.9289, 0.9244, 0.9192, 0.9130, 0.9131, 0.9164, 0.9098, 0.9042, 0.9092,
                                                0.9054, 0.9099, 0.9042, 0.9232, 0.9184, 0.9216, 0.9295, 0.9272, 0.9255,
                                                0.9232, 0.9238, 0.9174, 0.9111, 0.9036, 0.8971, 0.8915, 0.8957, 0.8922,
                                                0.8841, 0.8801, 0.8781, 0.8759, 0.8790, 0.8779, 0.8775, 0.8758, 0.8874,
                                                0.8930, 0.8977, 0.8950, 0.8879, 0.8946, 0.8873, 0.8818, 0.8811, 0.8741,
                                                0.8758, 0.8744, 0.8760, 0.8711, 0.8795, 0.8739, 0.8781, 0.8754, 0.8694,
                                                0.8695, 0.8668, 0.8630, 0.8791, 0.8756, 0.8732, 0.8709, 0.8630, 0.8558,
                                                0.8674, 0.8674, 0.8682, 0.8713, 0.8662, 0.8749, 0.8685, 0.8615, 0.8535,
                                                0.8476, 0.8419, 0.8344, 0.8288, 0.8265, 0.8209, 0.8151, 0.8085, 0.8173,
                                                0.8450, 0.8382, 0.8882, 0.8795, 0.8690, 0.8628, 0.8554, 0.8478, 0.8388,
                                                0.8337, 0.8274, 0.8181, 0.8069, 0.7931, 0.7867, 0.7759, 0.7628, 0.7569,
                                                0.7520, 0.7429, 0.7369, 0.7290, 0.7228, 0.7131, 0.7068, 0.7011, 0.6932,
                                                0.6802, 0.6713, 0.6617, 0.6552, 0.6438, 0.6372, 0.6335, 0.6219, 0.6154,
                                                0.6095, 0.6006, 0.5956, 0.5902, 0.5861, 0.5792, 0.5712, 0.5636, 0.5570,
                                                0.5494, 0.5433, 0.5371, 0.5313, 0.5240, 0.5311, 0.5260, 0.5200, 0.5141,
                                                0.4993, 0.4894, 0.4921, 0.4860, 0.4823, 0.4767, 0.4725, 0.4683, 0.4645,
                                                0.4609, 0.4569, 0.4526, 0.4489, 0.4445, 0.4301, 0.4271, 0.4243, 0.4201,
                                                0.4169, 0.4140, 0.4102, 0.4053, 0.4004, 0.3963, 0.3874, 0.3823, 0.3773,
                                                0.3738, 0.3697, 0.3660, 0.3627, 0.3601, 0.3567, 0.3504, 0.3477, 0.3449,
                                                0.3423, 0.3397, 0.3366, 0.3333, 0.3287, 0.3258, 0.3233, 0.3174, 0.3133,
                                                0.3050, 0.3028, 0.2987, 0.2916])
"""
norm = torchvision.transforms.Normalize(mean=[ 0.5533,  0.5563,  0.5872,  0.5788,  0.5771,  0.5916,  0.5669,  0.5585,
                                               0.5965,  0.5861,  0.6195,  0.6159,  0.5876,  0.6443,  0.5022,  0.5607,
                                               0.5544,  0.7234,  0.6282],
                                        std=[  0.4972, 0.4968, 0.4924, 0.4938, 0.4940, 0.4916, 0.4955, 0.4966, 0.4906,
                                               0.4926, 0.4855, 0.4864, 0.4923, 0.4788, 0.5000, 0.4963, 0.4971, 0.4473,
                                               0.4833])
"""


class Mish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)
        return x * ( torch.tanh(F.softplus(x)))


class DcModelInner(nn.Module):
    def __init__(self, delta_size):
        super(DcModelInner, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(delta_size * 4 + 1, 300),
            torch.nn.AlphaDropout(0.3),
            Mish(),
            torch.nn.Linear(300, 300),
            torch.nn.AlphaDropout(0.3),
            Mish(),
            torch.nn.Linear(300, 300),
            torch.nn.AlphaDropout(0.3),
            Mish(),
            #torch.nn.Linear(300, 300),
            #torch.nn.AlphaDropout(0.3),
            #Mish(),
            torch.nn.Linear(300, delta_size),
            torch.nn.Sigmoid()
        )

    def forward(self, x, prev_prediction):
        x = torch.cat((x, prev_prediction), dim=1)
        x = self.model.forward(x)
        return x


class DcModel(nn.Module):
    def __init__(self, delta_size, n_timesteps, device):
        super(DcModel, self).__init__()
        self.n_timesteps = n_timesteps
        self.inner = DcModelInner(delta_size)
        self.delta_size = delta_size
        self.device = device
        self.to(device)

    def forward(self, x):

        # print("x", x.shape)
        x = x[:, :, -1]
        print(x.shape)
        quit()
        x = norm.forward(torch.reshape(x, shape=(x.shape[0], x.shape[1], x.shape[2], 1))).squeeze(3)

        prediction = torch.empty((x.shape[0], self.delta_size, self.n_timesteps)).to(self.device)
        prediction[:, :, 0] = x[:, 0:self.delta_size, 0]
        # prediction.names = ['C', 'T']

        # prediction[:, :, 0] = self.inner.forward(x[:, :, 0], x[:, 0:19, 0])
        for step_idx in range(1, self.n_timesteps):
            # prediction[:, :, step_idx] = self.inner.forward(x[:, :, step_idx])
            prediction[:, :, step_idx] = self.inner.forward(x[:, :, step_idx], prediction[:, :, step_idx - 1])

        return prediction[:, :, self.n_timesteps - 1]

        """
        #print("x", x.shape)
        x = norm.forward(torch.reshape(x, shape=(x.shape[0], x.shape[1], x.shape[2], 1))).squeeze(3)
        
        prediction = torch.empty((x.shape[0], self.delta_size, self.n_timesteps)).to(self.device)
        prediction[:, :, 0] = x[:, 0:self.delta_size, 0]
        #prediction.names = ['C', 'T']

        #prediction[:, :, 0] = self.inner.forward(x[:, :, 0], x[:, 0:19, 0])
        for step_idx in range(1, self.n_timesteps):
            #prediction[:, :, step_idx] = self.inner.forward(x[:, :, step_idx])
            prediction[:, :, step_idx] = self.inner.forward(x[:, :, step_idx], prediction[:, :, step_idx - 1])

        return prediction[:, :, self.n_timesteps - 1]
        """

        """
        #print("x", x.shape)
        x = norm.forward(torch.reshape(x, shape=(x.shape[0], x.shape[1], x.shape[2], 1))).squeeze(3)

        prediction = torch.empty((x.shape[0], 19, self.n_timesteps)).to(self.device)
        prediction[:, :, 0] = x[:, 0:19, 0]
        #prediction.names = ['C', 'T']

        prediction[:, :, 0] = self.inner.forward(x[:, :, 0], x[:, 0:19, 0])
        for step_idx in range(1, self.n_timesteps):
            #print("1", x[:, :, step_idx])
            #print("2", prediction[:, :, step_idx - 1])
            #quit()
            prediction[:, :, step_idx] = self.inner.forward(x[:, :, step_idx], prediction[:, :, step_idx - 1])

        return prediction
        """


class DcDataset(Dataset):
    def __init__(self, features_np, targets_np, device):
        self.features = torch.from_numpy(features_np).float().to(device)
        self.targets = torch.from_numpy(targets_np).float().to(device)
        #self.targets.names = ['N', 'C', 'T']
        #self.features.names = ['N', 'C', 'T']

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx][:, -1]

    def __len__(self):
        return len(self.targets)



